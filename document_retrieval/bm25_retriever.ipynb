{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "\n",
    "lines = []\n",
    "for i in range(1,10):\n",
    "    with open(f\"/nfs/nas-6.1/wclu/AICUP/2023/aicup-2023-nlp/data/wiki-pages/wiki-00{i}.jsonl\", encoding=\"utf8\") as f:\n",
    "        lines.extend(f.read().splitlines())\n",
    "for i in range(10,25):\n",
    "    with open(f\"/nfs/nas-6.1/wclu/AICUP/2023/aicup-2023-nlp/data/wiki-pages/wiki-0{i}.jsonl\", encoding=\"utf8\") as f:\n",
    "        lines.extend(f.read().splitlines())\n",
    "\n",
    "wiki_df_0 = pd.DataFrame([json.loads(line) for line in lines])\n",
    "\n",
    "wiki_text = []\n",
    "lines = wiki_df_0['lines'].to_list()\n",
    "ids = wiki_df_0['id'].to_list()\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    for j in re.split(r\"\\n(?=[0-9])\", lines[i]):\n",
    "        j_list = j.split('\\t')\n",
    "        if len(j_list)>=2 and j_list[1]!='':\n",
    "            wiki_text.append([ids[i], j_list[0], j_list[1]])\n",
    "\n",
    "wiki_df = pd.DataFrame(wiki_text, columns=['id', 'line', 'text'])\n",
    "\n",
    "lines = []\n",
    "with open(f\"/nfs/nas-6.1/wclu/AICUP/2023/aicup-2023-nlp/data/public/public_train.jsonl\", encoding=\"utf8\") as f:\n",
    "    lines.extend(f.read().splitlines())\n",
    "train_df = pd.DataFrame([json.loads(line) for line in lines])\n",
    "\n",
    "lines = []\n",
    "with open(f\"/nfs/nas-6.1/wclu/AICUP/2023/aicup-2023-nlp/data/public/public_test.jsonl\", encoding=\"utf8\") as f:\n",
    "    lines.extend(f.read().splitlines())\n",
    "test_df = pd.DataFrame([json.loads(line) for line in lines])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = pd.read_csv('/nfs/nas-6.1/wclu/AICUP/2023/aicup-2023-nlp/data/tokenized_sent.csv')['0'].apply(lambda x: str(x).split()).tolist()\n",
    "tokenized_doc = pd.read_csv('/nfs/nas-6.1/wclu/AICUP/2023/aicup-2023-nlp/data/tokenized_doc.csv')['0'].apply(lambda x: str(x).split()).tolist()\n",
    "tokenized_query = pd.read_csv('/nfs/nas-6.1/wclu/AICUP/2023/aicup-2023-nlp/data/tokenized_claim.csv')['0'].apply(lambda x: str(x).split()).tolist()\n",
    "tokenized_query_test = pd.read_csv('/nfs/nas-6.1/wclu/AICUP/2023/aicup-2023-nlp/data/tokenized_claim_private.csv')['0'].apply(lambda x: str(x).split()).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_df_0['doc'] = tokenized_doc\n",
    "wiki_df['sent'] = tokenized_sent\n",
    "train_df['tokenized_query'] = tokenized_query\n",
    "test_df['tokenized_query'] = tokenized_query_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "bm25 = BM25Okapi(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491453398fdc4e76ad061b29b1664106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=805), Label(value='0 / 805'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_DOC = 10\n",
    "def predict_doc(query):\n",
    "    doc_scores = bm25.get_scores(query)\n",
    "    predict = wiki_df_0['id'].iloc[doc_scores.argsort()[::-1][:NUM_DOC]].tolist()\n",
    "    return predict\n",
    "\n",
    "#train_df['predicted_doc'] = train_df['tokenized_query'].parallel_apply(predict_doc)\n",
    "\n",
    "test_df['predicted_doc'] = test_df['tokenized_query'].parallel_apply(predict_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_pickle('data/test_private_doc100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "      <th>tokenized_query</th>\n",
       "      <th>predicted_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21498</td>\n",
       "      <td>雞形目的鳥腿腳強健，擅長在地面奔跑，其中有珍稀物種，體態雄健優美、顏色鮮豔；也有經濟物種，與...</td>\n",
       "      <td>[雞形目, 的, 鳥, 腿腳, 強健, ，, 擅長, 在, 地面, 奔跑, ，, 其中, 有...</td>\n",
       "      <td>[雞形目, 斯卡利亞壯麗鳥, 平腹蛛科, 非洲刺毛鼠屬, 耳烏賊目, 肉座菌目, 紅豆杉科,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13037</td>\n",
       "      <td>教會剛建立時為解決內部的一些問題，使徒們寫下許多便條，其中有八卷不是保羅寫的為大公書信。</td>\n",
       "      <td>[教會, 剛, 建立, 時, 為, 解決, 內部, 的, 一些, 問題, ，, 使徒, 們,...</td>\n",
       "      <td>[新約書信, 大公書信, 提多書, 歌羅西書, 羅馬書, 書信_(文體), 哥林多前書, 保...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18652</td>\n",
       "      <td>羅伯·昆蘭於明尼蘇達州聖保羅市出生。</td>\n",
       "      <td>[羅伯, ·, 昆蘭, 於, 明尼蘇達州, 聖保羅市, 出生, 。]</td>\n",
       "      <td>[羅伯·昆蘭, 基斯·高文_(政治人物), 聖保羅機場, 聖保羅國際機場, 聖保羅市, 羅伯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21378</td>\n",
       "      <td>2015年美國網球公開賽女子單打比賽裡小威廉絲是上一屆的冠軍。</td>\n",
       "      <td>[2015年, 美國, 網球, 公開賽, 女子, 單打, 比賽, 裡, 小, 威廉絲, 是,...</td>\n",
       "      <td>[2015年溫布頓網球錦標賽女子單打比賽, 2015年美國網球公開賽, 2016年美國網球公...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18624</td>\n",
       "      <td>南陽郡的地方豪族出身的漢光武帝劉秀為太祖劉邦的九世孫。</td>\n",
       "      <td>[南陽郡, 的, 地方, 豪族, 出身, 的, 漢, 光武帝, 劉秀, 為, 太祖, 劉邦,...</td>\n",
       "      <td>[漢光武帝, 樊嫺都, 劉讓_(臨邑侯), 劉欽_(南頓縣令), 更始帝, 劉信_(汝陰侯)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8044</th>\n",
       "      <td>12412</td>\n",
       "      <td>劉德華在中國星集團拍攝的最後一部電影爲大隻佬_(電影），之後他就結束與中國星集團14年賓主關係。</td>\n",
       "      <td>[劉德華, 在, 中國, 星, 集團, 拍攝, 的, 最後, 一, 部, 電影, 爲, 大隻...</td>\n",
       "      <td>[最愛女人購物狂, 大隻佬_(電影), 星劇本, 華夏電影, 賭城風雲III, 女子監獄, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8045</th>\n",
       "      <td>9611</td>\n",
       "      <td>視光學是利用光學儀器搭配侵入的方式來治療患者雙眼視覺異常。</td>\n",
       "      <td>[視光學, 是, 利用, 光學, 儀器, 搭配, 侵入, 的, 方式, 來, 治療, 患者,...</td>\n",
       "      <td>[視光學, 光學工程, 雙眼視覺, 耳面症候群, 立體視覺, 骨髓移植, 儀器飛行, 不可能...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8046</th>\n",
       "      <td>10538</td>\n",
       "      <td>婊子千層麪這首歌諷刺了印度音樂唱片公司 「 T Series 」 ， 用來回應T Serie...</td>\n",
       "      <td>[婊子, 千, 層, 麪, 這, 首, 歌, 諷刺, 了, 印度, 音樂, 唱片, 公司, ...</td>\n",
       "      <td>[婊子千層麪, PewDiePie與TSeries之爭, PewDiePie與T-Serie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8047</th>\n",
       "      <td>11757</td>\n",
       "      <td>兩棲動物由於在水中適應比不上腔棘魚，在陸地生存又落後羊膜動物，因此現今大多種類已絕種。</td>\n",
       "      <td>[兩棲, 動物, 由於, 在, 水, 中, 適應, 比不上, 腔棘魚, ，, 在, 陸地, ...</td>\n",
       "      <td>[兩棲動物, 羊膜動物, 合弓綱, 水生動物, 石炭蜥目, 巴基鯨科, 四足動物, 節胸屬,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8048</th>\n",
       "      <td>504</td>\n",
       "      <td>鐒是由阿伯特 · 吉奧索等人發現的。</td>\n",
       "      <td>[鐒, 是, 由, 阿伯特, ·, 吉奧索, 等, 人, 發現, 的, 。]</td>\n",
       "      <td>[鐒, 艾伯特·吉奧索, 牛頓阿伯特, 克里斯多福·阿伯特, 阿伯特_(得克薩斯州), 天主...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8049 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              claim  \\\n",
       "0     21498  雞形目的鳥腿腳強健，擅長在地面奔跑，其中有珍稀物種，體態雄健優美、顏色鮮豔；也有經濟物種，與...   \n",
       "1     13037       教會剛建立時為解決內部的一些問題，使徒們寫下許多便條，其中有八卷不是保羅寫的為大公書信。   \n",
       "2     18652                                 羅伯·昆蘭於明尼蘇達州聖保羅市出生。   \n",
       "3     21378                    2015年美國網球公開賽女子單打比賽裡小威廉絲是上一屆的冠軍。   \n",
       "4     18624                        南陽郡的地方豪族出身的漢光武帝劉秀為太祖劉邦的九世孫。   \n",
       "...     ...                                                ...   \n",
       "8044  12412   劉德華在中國星集團拍攝的最後一部電影爲大隻佬_(電影），之後他就結束與中國星集團14年賓主關係。   \n",
       "8045   9611                      視光學是利用光學儀器搭配侵入的方式來治療患者雙眼視覺異常。   \n",
       "8046  10538  婊子千層麪這首歌諷刺了印度音樂唱片公司 「 T Series 」 ， 用來回應T Serie...   \n",
       "8047  11757        兩棲動物由於在水中適應比不上腔棘魚，在陸地生存又落後羊膜動物，因此現今大多種類已絕種。   \n",
       "8048    504                                 鐒是由阿伯特 · 吉奧索等人發現的。   \n",
       "\n",
       "                                        tokenized_query  \\\n",
       "0     [雞形目, 的, 鳥, 腿腳, 強健, ，, 擅長, 在, 地面, 奔跑, ，, 其中, 有...   \n",
       "1     [教會, 剛, 建立, 時, 為, 解決, 內部, 的, 一些, 問題, ，, 使徒, 們,...   \n",
       "2                    [羅伯, ·, 昆蘭, 於, 明尼蘇達州, 聖保羅市, 出生, 。]   \n",
       "3     [2015年, 美國, 網球, 公開賽, 女子, 單打, 比賽, 裡, 小, 威廉絲, 是,...   \n",
       "4     [南陽郡, 的, 地方, 豪族, 出身, 的, 漢, 光武帝, 劉秀, 為, 太祖, 劉邦,...   \n",
       "...                                                 ...   \n",
       "8044  [劉德華, 在, 中國, 星, 集團, 拍攝, 的, 最後, 一, 部, 電影, 爲, 大隻...   \n",
       "8045  [視光學, 是, 利用, 光學, 儀器, 搭配, 侵入, 的, 方式, 來, 治療, 患者,...   \n",
       "8046  [婊子, 千, 層, 麪, 這, 首, 歌, 諷刺, 了, 印度, 音樂, 唱片, 公司, ...   \n",
       "8047  [兩棲, 動物, 由於, 在, 水, 中, 適應, 比不上, 腔棘魚, ，, 在, 陸地, ...   \n",
       "8048             [鐒, 是, 由, 阿伯特, ·, 吉奧索, 等, 人, 發現, 的, 。]   \n",
       "\n",
       "                                          predicted_doc  \n",
       "0     [雞形目, 斯卡利亞壯麗鳥, 平腹蛛科, 非洲刺毛鼠屬, 耳烏賊目, 肉座菌目, 紅豆杉科,...  \n",
       "1     [新約書信, 大公書信, 提多書, 歌羅西書, 羅馬書, 書信_(文體), 哥林多前書, 保...  \n",
       "2     [羅伯·昆蘭, 基斯·高文_(政治人物), 聖保羅機場, 聖保羅國際機場, 聖保羅市, 羅伯...  \n",
       "3     [2015年溫布頓網球錦標賽女子單打比賽, 2015年美國網球公開賽, 2016年美國網球公...  \n",
       "4     [漢光武帝, 樊嫺都, 劉讓_(臨邑侯), 劉欽_(南頓縣令), 更始帝, 劉信_(汝陰侯)...  \n",
       "...                                                 ...  \n",
       "8044  [最愛女人購物狂, 大隻佬_(電影), 星劇本, 華夏電影, 賭城風雲III, 女子監獄, ...  \n",
       "8045  [視光學, 光學工程, 雙眼視覺, 耳面症候群, 立體視覺, 骨髓移植, 儀器飛行, 不可能...  \n",
       "8046  [婊子千層麪, PewDiePie與TSeries之爭, PewDiePie與T-Serie...  \n",
       "8047  [兩棲動物, 羊膜動物, 合弓綱, 水生動物, 石炭蜥目, 巴基鯨科, 四足動物, 節胸屬,...  \n",
       "8048  [鐒, 艾伯特·吉奧索, 牛頓阿伯特, 克里斯多福·阿伯特, 阿伯特_(得克薩斯州), 天主...  \n",
       "\n",
       "[8049 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evi_doc(evidence_list):\n",
    "    if evidence_list[1] == None:\n",
    "        return None\n",
    "    e_list = []\n",
    "    for evidence in evidence_list[0]:\n",
    "        id = evidence[2]\n",
    "        if not(id in e_list):\n",
    "            e_list.append(id)\n",
    "    return e_list\n",
    "\n",
    "train_df_info['evidence_doc'] = train_df_info['evidence'].apply(evi_doc)\n",
    "\n",
    "def evi_sent(evidence_list):\n",
    "    if evidence_list[1] == None:\n",
    "        return None\n",
    "    e_list = []\n",
    "    for evidence in evidence_list[0]:\n",
    "        id = evidence[2]\n",
    "        line = evidence[3]\n",
    "        e_list.append(f'{id}_{line}')\n",
    "    return e_list\n",
    "\n",
    "train_df_info['evidence_line'] = train_df_info['evidence'].apply(evi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "total = len(train_df_info)\n",
    "for NUM_PAGE in tqdm(range(NUM_DOC)):\n",
    "    wrong = 0\n",
    "    for i in range(total):\n",
    "        for e in train_df_info['evidence_list'][i]:\n",
    "            if not(e in train_df_info['predicted_pages'][i][:NUM_PAGE]):\n",
    "                wrong += 1\n",
    "                break\n",
    "    score.append(round((total-wrong)/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    print(f'{i}_{score[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_from_doc(docs):\n",
    "    sents = []\n",
    "    ids = []\n",
    "    for doc in docs:\n",
    "        ind = (wiki_df['id']==doc)\n",
    "        sents.extend(wiki_df[ind]['sent'].tolist())\n",
    "        id = wiki_df[ind]['id'].astype('str')\n",
    "        line = wiki_df[ind]['line'].astype('str')\n",
    "        ids.extend((id+'_'+line).tolist())\n",
    "    return sents, ids\n",
    "\n",
    "def rank_sent(query_sents_ids):\n",
    "    bm25 = BM25Okapi(query_sents_ids[1])\n",
    "    predict = bm25.get_top_n(query_sents_ids[0], query_sents_ids[2], n=len(query_sents_ids[2]))\n",
    "    return predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_info['predicted_sents'] = train_df_info['predicted_pages'].parallel_apply(sent_from_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_info = pd.read_pickle('data/train_df_info_doc2sent.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sents_ids = []\n",
    "for i in tqdm(range(len(train_df_info))):\n",
    "    query_sents_ids.append([train_df_info['tokenized_query'][i], train_df_info['predicted_sents'][i][0], train_df_info['predicted_sents'][i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_info['rerank_sents'] = pd.Series(query_sents_ids).parallel_apply(rank_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "total = len(train_df_info)\n",
    "NUM_SENT=1000\n",
    "for num_sent in tqdm(range(NUM_SENT)):\n",
    "    wrong = 0\n",
    "    for i in range(total):\n",
    "        l = len(train_df_info['rerank_sents'][i])\n",
    "        for e in train_df_info['evidence_line'][i]:\n",
    "            m = min(num_sent, l)\n",
    "            if not(e in train_df_info['rerank_sents'][i][:m]):\n",
    "                wrong += 1\n",
    "                break\n",
    "    score.append(round((total-wrong)/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(500):\n",
    "    print(f'{i}_{score[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_pickle('data/train_df_doc100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evi_doc(evidence_list):\n",
    "    if len(evidence_list[0])>1 and evidence_list[0][1] == None:\n",
    "        return None\n",
    "    e_list = []\n",
    "    for evidence in evidence_list[0]:\n",
    "        id = evidence[2]\n",
    "        if not(id in e_list):\n",
    "            e_list.append(id)\n",
    "    return e_list\n",
    "\n",
    "train_df['ground_doc'] = train_df['evidence'].apply(evi_doc)\n",
    "\n",
    "def evi_sent(evidence_list):\n",
    "    if len(evidence_list[0])>1 and evidence_list[0][1] == None:\n",
    "        return None\n",
    "    e_list = []\n",
    "    for evidence in evidence_list[0]:\n",
    "        id = evidence[2]\n",
    "        line = evidence[3]\n",
    "        e_list.append(f'{id}_{line}')\n",
    "    return e_list\n",
    "\n",
    "train_df['ground_sent'] = train_df['evidence'].apply(evi_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['id','label','claim','evidence','ground_doc','ground_sent', 'predicted_pages']].to_pickle('data/train_doc100.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
